\documentclass[onecolumn,           % Format : preprint, twocolumn
%\documentclass[preprint,           % Format : preprint, twocolumn
               showpacs,            % Pacs : showpacs, noshowpacs
               preprintnumbers,     % Preprint: preprintnumbers,
               			    %           nopreprintnumbers
               aps,                 % Society: ...
               prl,          	    % Journal Style : pra, prb, prc, prd, pre,
               			    %                 prl, prstab, rmp
               letterpaper,             % Size : a4paper, ...
               superscriptaddress,      % Affiliation (Title) : groupedaddress,
                                    %                       superscriptaddress,
                                    %                       unsortedaddress
               nofootinbib,         % Footnote: footinbib, nofootinbib
               tightenlines,        % Remove additional spaces in a line
               floats,floatfix      % Floating pictures and tables
               ,usenatbib,
               ]{revtex4-1}
\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables}
%\usepackage[style=authoryear,backend=biber]{biblatex}
\usepackage{bm}        % for math
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{color}
\definecolor{purple}{rgb}{0.58,0.0,0.83}
\usepackage{caption}
\usepackage[toc,page]{appendix}

\begin{document}

\title{An introduction on bayesian statistics and its applications in cosmology}
\author{Luis Padilla-Albores}  
\email{epadilla@fis.cinvestav.mx}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
   \author{Alberto Vazquez-Gonzalez}  
\email{vetovazquez@hotmail.com}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
  \author{Tonatiuh Matos-Chassin}  
\email{tmatos@fis.cinvestav.mx}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
\date{\today}

\begin{abstract}

In this paper we review the basic concepts on Bayesian statistics for parameter inference and how we can use it in cosmology. This work is organized in such a way that if the reader is only interested in the parameter inference procedure for a model, but not in the cosmological part, he/she can make use of it ignoring only the last part of the paper. 

First, we start giving the basic differences between Bayesian and Frequentist statistics. Then we continue with a no so formal introduction on the basic mathematical concepts necessary for a Bayesian parameter inference procedure. Later, we review the most common computational tools at our disposal that can help us to simplify this task. Finally we show how this new concepts can be applied in cosmology.     
\end{abstract}

%\pacs{????}
\maketitle

\section{Introduction}

The begging of the standard cosmology as it is known today emerge afterwards 1920 when the Shaphey-Curtis debate was carry out \cite{debate}. This debate was held between the astronomers Harlow Shapley and Heber Curtis, resulting in a revolution for astronomy at that time: ``The universe had a larger scale than the Milky Way galaxy". Several observations at that epoch established the size and the dynamics of the cosmos that could only be explained by Einstein's General Theory of Relativity. In its infancy, Cosmology was a speculative science that was based only on a few set of data and characterized by a dispute between two cosmological models: the steady state model and the Big Bang (BB) theory. It was not until 1990s when the amount of data was enough to eliminate competitive theories, being awarded the BB model as the most accepted cosmological theory. In that same decade David Schramm herald the ``Golden Age of Cosmology" at a National Academy of Sciences colloquium.    

Once the new age of cosmological experiments arrived with a very large variety of cosmological data, it was necessary to confront all the cosmological models with those data. The way that it is usually done is using a statistical confrontation. First at all we need to notice that, because we only have one Universe, we can not consider a frequentist interpretation of statistics (we can not create multiple Universes to make a frequentist inference of our models). An alternative interpretation in which we can help us is the Bayesian statistics. On this alternative, probability is interpreted as a degree of belief and it can be useful when no repetitive processes need to be considered.  

The principal objective of this work is to give to the reader a first step on bayesian parameter inference and how we can use it in cosmology. We consider that the reader is familiarized with the basic concepts of statistics, but not necessarily with Bayesian statistics. Then, we give a not so formal panorama of it, enough to understand the basics concepts of our main objective. This panorama is written in a generic way in  such case that, if the reader is not interested in the cosmological section but in the parameter inference one, he/she can draw on the bayesian part.  

This paper is organized as follows. We start in section [Bayesian vs frequentist statistic] mentioning the most important differences between the Bayesian statistics and the Frequentist one. Then, in section [A first look on the Bayesian statistics] we present the basic mathematical concepts in Bayesian statistics that are going to be necessary at the moment that we want to estimate a model parameter. Once we have the mathematical concepts, we continue in section [Numerical tools] with the numerical tools at our disposal that can help us to simplify our homework. Such numerical tools are so important given the fact that, in general, it is not possible to apply the mathematical ones analytically when several par√°meters of our models need to be confronted with data. In section [Bayesian statistics and cosmology] we show how this tools can be used in cosmology, mentioning the different numerical codes free to download on web that are programed to do this homework and applying them to specific examples. Finally, in section [Conclusions] we conclude this paper.

\section{Bayesian vs frequentist statistics}

In this section we introduce the basics to understand the differences about bayesian and frequentist statistics. For an extended review see \cite{bayeslecture}, \cite{AlanH}, \cite{RobT}, \cite{LiV} or/and \cite{RobTr}. 

Fundamentally, the main difference between bayesian and frequentist is its definition of probability. In frequentist statistic probability has mining in a limiting case of repeating mesurements
\begin{equation}
P=\frac{n}{N}
\end{equation}
where $n$ denotes the number of successes and $N$ the total number of trials. Frequentist define probability as the limit for the number of independent trials going to infinity. Then, \textbf{for frequentist, probabilities are fundamentally related to frequencies of events}. On the other hand in bayesian statistic the concept of probability is extended to cover degrees of certainty about statement. \textbf{For Bayesians, probabilities are fundamentally related to our own knowledge about an event}.

If we consider that $x$ is a random variable related with a particular event and $P(x)$ its corresponding probability distribution, for both cases the same rules of probabilities apply, which are\footnote{This rules are defined for a continuous variable. However, the corresponding discreet definition can be given immediately by replacing $\int \rightarrow \sum$.}
\begin{subequations}\label{rules}
\begin{equation}\label{rule1}
P(x)\geq 0
\end{equation}
\begin{equation}\label{rule2}
\int_{-\infty}^\infty dxP(x)=1\end{equation}
For mutually exclusive events \begin{equation}\label{rule3}
P(x_1\cup x_2)=P(x_1)+P(x_2)
\end{equation}
In general 
\begin{equation}\label{rule4}
P(x_1,x_2)=P(x_1)P(x_1|x_2)\end{equation}
\end{subequations}
In words, the last rule can be explain as: the probability of $x_1$ and $x_2$ to happen is the probability of $x_1$ times the conditional probability of $x_2$ given that $x_1$ has already happened. \\
%But what does the last rules means? The first condition is necessary because we want that the probability of having an event is always positive. The second rule is a normalized relation which tell us that whatever event we have, it will always be a corresponding probability in the model that we are working with. Now, in the third point we have a relation about two mutually exclusive events which can be reed as follows: if $x_1$ and $x_2$ are two measurements one independent of the other, then 

\subsection{Frequentist statistics}

Any frequentist inferential procedure relies on three basic ingredients: the data, a model and an estimator procedure. The central assumption in frequentism is that the data has a definite but unknown, underlying distribution to which all inference pertains.

The \textit{data} is a mesurement or observation which we denote by $X$, taking values in a corresponding sample space. Then, a \textit{sample space} of an observation $X$ can be defined as a measurable space $(x,\hat B)$ containing all values that $X$ can take upon measurement.

In frequentist statistics it is considered that there is a probability function $P_0:\hat B\rightarrow [0,1]$ on the sample space $x$ representing the `` true distribution of the data"
\[X\sim P_0\]

The second ingredient mentioned above is the model. For frequentist statistics a model $Q$ is a collection of probability measures $P:\hat B\rightarrow[0,1]$ on the sample space $(x,\hat B)$. The distribution $P_\theta$ are called model distributions. In this approach $\theta$ is unchanged.  Of course $P_\theta(x)$ have to follow the rules mentioned at the beginning of this section.

A model $Q$ is said to be well-specified if it contains the true distribution of the data $P_0$, i.e.
\[P_0\in Q\]

Finally we need a point-estimator(or estimator) for $P_0$. An estimator for $P_0$ is a map $\hat P:x\rightarrow Q$, representing our "best guess" $\hat P\in Q$ for $P_0$ based on the data $X$.

Hence, the frequentist statistic is based in trying to response the following questions: "what does the data make clear about $P_0$?", "from the data, what can we say about the mean of $P_0$?",  etcetera.

\subsection{Bayesian statistics}

As is explained in \cite{bayeslecture} in bayesian statistics, data and model form two factors of the same space, i.e. no formal distinction is made between measured quantities $X$ and parameters $\theta$. One may envisage the process of generating a measurement outcome $Y=y$ as two draws, one draw for $\Theta$ (or $Q$)
to select a value of $\theta$ (or distribution $P_\theta$) and a subsequent draw for $P_\theta$ to arrive at $X=x$. This perspective may seem rather absurd in view of the definitions made in the subsection above. In this way an element $P_\theta$ of the model is interpreted simply as the distribution of $X$ given the parameter value $\theta$, i.e. as the conditional distribution $X|\theta$.
\subsection{A little more clear}

It is so important to understand what are the differences between both kind of approximations. For it, let us review with an example in this subsection the basic differences about the frequentist and bayesian statistics.
\begin{table}[h!]
\centering
\begin{tabular}{||l|l||} 
 \hline
 \textbf{Frequentist} & \textbf{Bayesian} \\ [0.5ex] 
 \hline\hline
 Data are a repeatable random  & Data are observed from the   \\ 
 sample. There is a frequency & realized sample the realized \\
 \hline 
 Underlying parameters remain & Parameters are unknown and \\
 constant during this repeatable & described probabilistically \\
 process &  \\
\hline
Parameters are fixed & Data are fixed\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Main differences between the bayesian and frequentist interpretations.}}
\label{table:1}
\end{table}

In table \ref{table:1} we have a very little review of the most important differences about the two approximations. In the other side in the next example we present an experiment seen in both points of view. Because we are interested in knowing both descriptions we show only the basic results of the procedures and we analyze the results in the point of view of the frequentist and bayesian statistics. 

\textit{Example.-} In this experiment  you have a coin that has a probability $p$ that when is through, it is head and a probability $1-p$ to be tall. Then, trying to estimate $p$ you flip the coin 14 times (spoiler alert: p=0.5). The result is head 10 times. Now, what we are interested about is in the next two possible events. To be precise: ``What is the probability that in the next two tosses we will get two heads in a row?"
\begin{itemize}
\item \textit{Frequentist approximation}. As we mentioned before in frequentist statistics probability is related with a frequency of events. In this way we have that our best estimate for $p$ is $P(head)=$No.heads/No.total$=10/14$. So, the probability to have 2 heads in a row is $P(2heads)=P(head)P(head)\simeq 0.51$.  
\item \textit{Bayesian approximation}. Now in the bayesian approach $p$ is not a value, it is a random variable with its own distribution. The distribution must be defined by the existing evidence. In this example a good distribution for $p$ is a binomial distribution. Then, by considering a uniform distribution as our prior information (we do not know anything about $p$) we have that the probability of having two heads is
\[
P(2heads|D)=\frac{B(13,5)}{B(11,5)}=0.485
\]
where $B(x,y)$ is the beta function.
\end{itemize}

We can see that both approximations arrive at different results. In one of them, since we consider probability as a frequency of events we have a bigger probability to have two heads in a row than when we consider probability as our knowledge about the experiment. However, in both cases the probability defers from the real one ($P(2heads)=0.25$) because we have not enough data for our estimations.

\textit{Note}: If you are not familiarized with bayesian statistics,  please be not scared of this last example. In the next section we will introduce the basic concepts to understand the bayesian part of it and then we will go back to this example and we will solve it using the new tools learned.   

%\subsection{Summary}

%In this section we reviewed the basic differences between the Bayesian and the Frequentist interpretation of statistics. Basically, what is important here is to notice that probability has a different meaning for a frequentist statistician than for a bayesian one. While for a Frequentist, probabilities are related to frequencies of events, for Bayesians, probabilities are related to our own knoledge about an event. Then, given this basic difference, there are several other ones that are a result of it and that are well summarized in table  \ref{table:1}. 

\section{A first look of the Bayesian statistics}

Before to start with the applications of bayesian statistics in cosmology it is necessary to know the most important tools for the bayesian procedure. In this section we review them in an informal way, keeping in mind that more formal readers can look for the formal treatment in the literature.   

\subsection{Bayes theorem, prior and posterior distributions}\label{BTPP}

When a statistician is interested in working in the Bayes framework there are 3 concepts that are necessary to understand before to interpret whatever result that he could obtain: the Bayes theorem, priors and posterior distributions. In this section we quickly review this concepts and then we take back the example about the coins given in the last section in order to exemplify them. 

\textit{The Bayes theorem.} The Bayes theorem is a direct consequence of the axioms of probability \eqref{rules}. We can see that from \eqref{rule4} without lost of generality we can rewrite $P(x_2,x_1)=P(x_2)P(x_2|x_1)$. But, of course, one expect that the relation $P(x_1,x_2)=P(x_2,x_1)$ have to be fulfill. With this we arrive at the Bayes formula   
\begin{equation}
P(x_2|x_1)=\frac{P(x_2)P(x_1|x_2)}{P(x_1)}
\end{equation}
As we mentioned before, in the Bayes framework, data and model are part of the same space. In this way considering $x_1\rightarrow D$ as a set of data, and $x_2\rightarrow H$ as our "hypothesis" we can rewrite the above equation in the Bayesian statistics without lost of generality as
\begin{equation}\label{BayesT}
P(\theta,H|D)=\frac{P(\theta,H)P(D|\theta,H)}{P(D)}
\end{equation}
Here we have added the extra term $\theta$ in order to specify that our hypothesis can depend on several parameters. 
The above equation is the so-called \textit{Bayes theorem} and it is the principal tool in a Bayesian inference procedure. In this result $P(\theta,H|D)$ is called the \textit{posterior} probability for the model. $L(D;\theta)\equiv P(D|\theta,H)$ is called the \textit{Likelihood} and we will concentrate on it in a future section. $P(D|\theta,H)$ is called the \textit{prior}, and expresses what we know about our model before to know the data. This prior can be fixed depending on previous experiment results or the theory that we are working with. $P(D)$ is the evidence of our model. We can notice that this evidence acts only as a normalization factor
\begin{equation}\label{PD}
P(D)=\int d\theta P(D|\theta,H)P(\theta,H).
\end{equation}
Then, this quantity is usually ignored for practical proposes when the parameter space of a unique model is tested. In the other side if what we are interested about is in a comparative between 2 or more models, the evidence plays an important role when we choose which model is more likely, whatever the parameters are. For the propose of this work we are not interested in this scenario and we will ignore it every time.

We can notice here that we have added a new ingredient in our Bayes description, which is the hypothesis. An hypothesis is our best guess of the model that best matches with the data, $H=Q_{best}$.

We can see that Bayes theorem has an enormous implication respect to an statistical inferential point of view. In a typical scenario we can collect some data and wish to interpret them in terms of a model. However, what we can usually do is the opposite, it is, we have first a set of data and then we can confront a model considering what is the probability that our model matches with the data. As we can see from \eqref{BayesT} Bayes theorem gives us a tool where we can relate both scenarios. Then, thanks to the Bayes theorem, in principle, we can know what is the "real" model that best fixes the data. 

\textit{Example}.- We consider again the example shown in the last section. What we are interested in is knowing what is the probability $P(2heads|D)$ ($D=$ our 14 data) to have 2 heads in a row given the 14 data that we already have. For simplicity we do not update $p$ between the two tosses and we assume that both tries are independent. To obtain the result that we expect we need to calculate
\begin{equation}\label{ex}
P(2heads|D)=\int^1_0 P(2heads|p)P(p|D)dp
\end{equation}
where $P(p|D)$ is the probability of the data given our model $p$ and $P(2heads|p)$ is the probability of our model $p$ given that we had 2 heads. 

Because we are considering that both tries are independent we have
\begin{equation}
P(2heads|p)=[P(head|p)]^2
\end{equation}
where $P(head|p)$ is the probability of our model $p$ given that we obtained a head. A good model for our experiment given the data is a binomial distribution
\begin{equation}\label{ex2}
P(D|p)=\binom{14}{10}p^{10}(1-p)^4
\end{equation}
Then 
\begin{equation}\label{ex1}
P(head|p)=p\Rightarrow P(2head|p)=p^2
\end{equation}
Now we need to compute $P(p|D)$. Using the bayes theorem we have
\begin{equation}
P(p|D)=\frac{P(D|p)P(p)}{P(D)}
\end{equation}

A very convenient prior distribution for this scenario is the \textit{beta distribution} $Beta(p;a,b)$ defined as
\begin{equation}\label{ex3}
Beta(p;a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}p^{a-1}(1-p)^{b-1}
\end{equation}
where $\Gamma$ is the gamma function. So
\begin{equation}\label{ex4}
P(p)=Beta(p;a,b)
\end{equation}
We are interested in the explicit form of $P(p|D)$ in such case we need to compute $P(D)$. Introducing \eqref{ex2} and \eqref{ex4} into \eqref{PD} we have
\begin{equation}
P(D)=B(10+a,4+b)\equiv \frac{\Gamma(10+a)\Gamma(4+b)}{\Gamma((10+a)+(4+b))}
\end{equation}
and then
\begin{equation}\label{ex4}
P(p|D)=\frac{p^{10+a-1}(1-p)^{4+b-1}}{B(10+a,4+b)}
\end{equation}

Now what we need to know are the values of $a$ and $b$. If we assume that we do not know anything about $p$ we can consider our prior as a normal distribution which corresponds to take $a=b=1$. 
Notice from figure \ref{coin0} that our posterior result \eqref{ex4} does not agree with the real result that we know (p=0.5). What we would like to expect is that our posterior distribution is centered at $p=0.5$ with a very thin distribution. Of course, this disagreement could be fixed if we continue increasing our experimental results.\\ $ $\\
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{coin0.png}
\captionof{figure}{\footnotesize{Prior and posterior distribution in the coin example. The black line corresponds with the well known real value of $p$.}}
\label{coin0}
\end{minipage}
\\$ $ \\
Solving the integral in \eqref{ex} with \eqref{ex1} and \eqref{ex4} we arrive at the final result obtained in the above section
\begin{equation}
P(2heads|D)=\frac{B(13,5)}{B(11,5)}=0.485
\end{equation}
%
%
%
\subsection{Updating the probability distribution for a model} 

As we saw in the coin example, due to the fact that we have not enough data we did not arrive to the real value of $p$. If we would like to be closer to the real one we should continue flipping the coin until the amount of data were enough. If we continue with the coin example and, we say, after throwing 100 times the coin we obtain 56 heads, and then after throwing 500 times the coin we obtain 246 heads we should obtain a more tiny distribution centered near $p=0.5$ (see fig. \ref{coin1}). In this way, it is clear that in order to confront a parameter model and be more accurate of the most probable (``real") value of it, it is necessary to increase the amount of data (and the capacity for precision) in any experimentation. For example, in the cosmological context, there are a lot of experiments like the ones associated with the barionic acoustic oscilations (BAO), the cosmic microwave background (CMB), the neutrinos background radiation, the redshift survey, etc. (if you are interested in knowing a pedagogical explanation of the different experiments, you can look for \cite{observ}), that can help us to confront our model's parameters (or models) and discriminate them in a more accurately way. Then, what we have here is that we will have some model's parameters that has to be confronted with different set of data. There are two ways we can do this: first is regarding the sum of all the sets of data that we have, or consider each data set as the new data, but our prior information has to be set by what we know about the previous set of data. What is important in bayes statistic is that it doesn't matter wich of the 2 possibilities we choose. In the coin context it means that it is equivalent to start with the prior given in figure \ref{coin1}-a and considering the 500 data we can arrive at the posterior \ref{coin1}-d or start with \ref{coin1}-c as our prior and consider the last 400 data to obtain the same posterior \ref{coin1}-d. \\ $ $ \\
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=10cm]{coin1.png}
\captionof{figure}{\footnotesize{Posterior distributions of $p$ when our data is increased. Notice that while we continue increasing the experimental results, the posterior distribution starts to be more localized near the real value $p=0.5$.}}
\label{coin1}
\end{minipage}
\\$ $ \\

In fact, if we rewrite Bayes theorem putting explicitly that all probabilities are conditional of some prior information $I$
\begin{equation}\label{BayesTI}
P(\theta,H|DI)=\frac{P(\theta,H|I)P(D|\theta,HI)}{P(D|I)}
\end{equation}
and then we consider a new set of data $D'$, letting the old data become part of the prior information $I'=DI$ we arrive at \cite{AlanH}   
\begin{equation}
P(\theta,H|DI')=\frac{P(\theta,H|I)P(DD'|\theta,HI)}{P(DD'|I)}=P(\theta,H|[DD']I)
\end{equation}
where we can see explicitly the equivalence of the 2 different options for analyzing our results. 
\subsection{About the Likelihood}

As we mentioned before the evidence in the Bayes theorem is not usually important when we try to do any inference procedure in the parameter space of a model. Then, we can fix it as $P(D)=1$ without lost of generality. In this way, if we ignore the prior\footnote{It is expected that the real value of a given parameter is independent of the election of the prior.} we can identify the likelihood with $P(H|D)$ and thus by maximizing it we can find the most likely model (or model's parameters) given the data. However having ignored $P(D)$ and the prior this approach cannot give in general a goodness of fit and thus cannot give an absolute probability for a given model. However it can give relative probabilities. In the other side, it is possible to report results independently of the prior by using the \textit{Likelihood ratio}. The likelihood at a particular point in parameter space can be compared with the one that best fit the observations, $L_{max}$. Then we can say that a model is acceptable if the likelihood ratio
\begin{equation}
\Lambda=-2\ln\left[\frac{L(D;\theta,H)}{L_{max}}\right]
\end{equation}
is bigger than a given value.

Let us assume we have a posterior distribution, which is single-picked. We consider that $\hat \theta$ corresponds with the peak of the distribution (most probable) or the mean
\begin{equation}
\hat \theta =\int d\theta \theta P(\theta,H|D).
\end{equation}
We consider that our model is well-specified and that the expectation value of $\hat \theta$ corresponds with the real value $\theta_0$
\begin{equation}
\langle\hat \theta\rangle=\theta_0,
\end{equation}
then we say that $\hat \theta$ is \textit{unbiased}. Considering a Taylor expansion of the log likelihood
\begin{equation}
\ln L(D;H)=\ln L(D;H_0)+\frac{1}{2}(\theta_\alpha-\theta_{0\alpha})\frac{\partial^2\ln L}{\partial\theta_\alpha \partial\theta_\beta}(\theta_\beta-\theta_{0\beta})+...
\end{equation}
where $H_0$ corresponds with the real model (and/or parameter of the model). In this way we have that the likelihood can be expressed as a multi-variable likelihood given by 
\begin{equation}\label{GLik}
L(D;H)=L(D;H_0)\exp \left[-\frac{1}{2}(\theta_\alpha-\theta_{0\alpha})H_{\alpha\beta}(\theta_\beta-\theta_{0\beta})\right]
\end{equation}
where 
\begin{equation}
H_{\alpha\beta}=\frac{\partial^2\ln L}{\partial\theta_\alpha \partial\theta_\beta}
\end{equation}
is called the \textit{Hessian matrix} and it controls whether the estimates of $\theta_\alpha$ and $\theta_\beta$ are correlated or not. If it is diagonal, the estimates are uncorrelated.

The above expression for the likelihood is a good approximation always that our posterior distribution possess a single-pick. However in a general way the likelihood may not be well described by a gaussian expression at levels wich set the interesting credibility levels. It is worth mentioned that if the data error are Gaussianly distributed, then the likelihood for the data will be a Gaussian function, as well.  In fact, this is always true if the model depends linearly on the parameters \cite{LiV}. In the other side, if the data are not Gaussianly distributed we can resort to the central limit theorem. In this way the central limit theorem will tell us that the resulting distribution will be better approximated by a multi-variate Gaussian \cite{LiV}.
\subsection{Justifying the neglect of the priors}

In this section we are interested in justifying the neglect of the prior in the Bayes theorem. For this, we follow the example given in \cite{RobT}. In this example there are 2 people A and B that are interested on the measurement of a given physical quantity $\theta$. A and B have different prior beliefs regarding the possible value of $\theta$. This discrepancy could be given by the experience, such as the possibility that A and B have made the same measurement in another moment being alone. Let us denote their priors by $P(\theta|I_i)$ $(i=A,B)$ and let us assume that they are described by two Gaussian distributions on mean $\mu_i$ and variance $\Sigma_i^2$. Now, A and B make a measurement together and they obtain the value $\theta_0=m_1$. If we consider that the experiment is such as that our parameters are uncorrelated we can rewrite \eqref{GLik} as
\begin{equation}\label{LikG}
L(D;HI)=L_0\exp\left[-\frac{1}{2}\frac{(\theta-m_1)^2}{\sigma^2}\right]
\end{equation}
Replacing the hypothesis $H$ by the continuous variable $\theta$ the Bayes theorem for the model of A and B is
\begin{equation}
P(\theta|m_1)=\frac{L(m_1;\theta I_i)P(\theta|I_i)}{P(m_1|I_i)}
\end{equation}
where we have used the notation given in \eqref{BayesTI}. Then the posterior of A and B are again Gaussian with means
\begin{equation}
\hat \mu_i = \frac{m_1+(\sigma/\Sigma_i)^2\mu_i}{1+(\sigma/\Sigma)^2}
\end{equation}
and variance 
\begin{equation}
\tau_i^2=\frac{\sigma^2}{1+(\sigma/\Sigma_i)^2}, \ \ (i=A,B)
\end{equation}
Thus if the likelihood is more informative than the prior i.e. $(\sigma/\Sigma)\ll 1$ the posterior means of A and B will converge towards the measured value, $m_1$. As more and more data are obtained one can simply replace the value of $m_1$ in the above equation by the mean $\langle m\rangle$ and $\sigma^2$ by $\sigma^2/N$. Then, we can see that the initial prior $\mu_i$ of A and B will progressively be overridden by the data. This process is ilutrated in figure \ref{gausian1} 
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=4.5cm]{g1.png}
\includegraphics[height=4.5cm]{g2.png}
\captionof{figure}{\footnotesize{Converging views in Bayes inference, taked from \cite{RobT}. A and B have different priors $P(\theta|I_i)$ about a value of $\theta$ (panel (a)). Then, they observe one datum with likelihood $L(\theta;HI)$ (panel (b)), after which their posteriors $P(\theta|m_1)$ (panel (c)) represents their updates. Then, after observing 100 data it can be seen how both posteriors are practically indistinguishable (panel (d))}}
\label{gausian1}
\end{minipage}
\subsection{Chisquare and goodness of fit}

We mentioned that it is necessary to maximize the likelihood in order to obtain the more likely model (or model parameters) given the data. If we consider the Gaussian approximation given in \eqref{LikG} we can see that the likelihood will be maximum always that the quantity
\begin{equation}\label{chi2}
\chi^2\equiv(\theta_\alpha-\theta_{0\alpha})H_{\alpha\beta}(\theta_\beta-\theta_{0\beta})
\end{equation}
is minimum. The quantity $\chi^2$ is usually called \textit{chisquare} and is related with the Gaussian likelihood via $L=L_0e^{-\chi^2/2}$. In this way we can say that the maximizing of a Gaussian likelihood procedure and the minimizing of a chisquare procedure are equivalent. However, as we mentioned before, there are some circumstances where the likelihood can not be well specified by a Gaussian distribution, then, in that cases the chisquare and the likelihood are not longer equivalent. 

We can consider a probability distribution for different values of $\chi^2$ around its minimum. This is the $\chi^2$ distribution for $v=n-M$ degrees of freedom where $n$ is the number of independent data points and $M$ the number of parameters. In this way we can calculate the probability that an observed $\chi^2$ exceeds by chance a value $\hat \chi$ for the correct model. This probability is given by \cite{NR} $Q(v,\hat\chi)=1-\Gamma(v/2,\hat\chi/2)$ where $\Gamma$ is the incomplete Gamma function. Then, the probability that the observed $\chi^2$ (even the correct model) is less that a given value $\hat\chi^2$ is $1-Q$. This statement is strictly true if errors are Gaussian and the model is a linear function of the likelihood, i.e., for Gaussian likelihoods.

If we evaluate the quantity $Q$ in the best fit of the chisquare (i.e. its minimum) we can have a measure of the goodness of the fit. If $Q$ is small (small probability) we can interpret it as:
\begin{itemize}
\item The model is wrong and can be rejected
\item The errors are underestimated 
\item The measurement errors are not Gaussianly distributed.
\end{itemize}
In the other side if $Q$ is too large there are some reasons that could cause such overestimation:
\begin{itemize}
\item Errors have been overestimated
\item Data are correlated of non-independent
\item The distribution is non-Gaussian.
\end{itemize}
\subsection{Contour plots and confidence regions}

Once the best fit parameters are obtained we would like to know if there are confidence regions where other parameters could be consider as a good candidate for our model. The most logical election is to consider parameters inside a compact region around the best fit value. Then, a natural choice is consider regions with a constant $\chi^2$ boundaries. In the case that $\chi^2$ posses more than 1 minimum it is say that we have more than one non-connected confidence region. For multi-variate Gaussian distributions (as the likelihood approximation \eqref{LikG}) these are ellipsoidal regions. In this section we exemplify how to calculate the confidence regions following \cite{LiV}. 

We can consider a little perturbation from the best fit of chisquare $\Delta\chi^2=\chi^2-\chi^2_{best}$. Then we can use the properties of $\chi^2$ distribution to define confidence regions for variations on $\chi^2$ to its minimum. In Table \ref{tableerrors} we can see the typical $68.3 \%$, $95.4\%$ and $99.5\%$ confidence levels as a function of number of parameters for the joint confidence level. In the case of Gaussian distribution (as the likelihood) these correspond to the conventional 1, 2 and 3 $\sigma$.
\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l|l||} 
 \hline
$\sigma$ & $p$ & $M=1$ & $M=2$ & $M=3$\\
\hline
$1\sigma$ & $68.3 \%$ & $1.00$ & $2.30$ & $3.53$\\
$2\sigma$ & $95.4 \%$ & $4.00$ & $6.17$ & $8.02$\\
$3\sigma$ & $99.73\%$ & $9.00$ & $11.8$ & $14.20$\\
\hline
\end{tabular}
\caption{\footnotesize{$\Delta \chi^2$ for the conventional $68.3\%$, $95.4\%$ and $99.73\%$ as a function of the number of parameters for the joint confidence level.}}\label{tableerrors}
\end{table}

The general cooking recipe to compute constant-$\chi^2$ confidence regions are as follow: After having found the best fit by minimizing $\chi^2$ (or maximizing the likelihood) and if $Q$ for the best parameters is acceptable, then:
\begin{enumerate}
\item Let $M$ be the number of parameters, $n$ the number of data and $p$ be the confidence limit desired.
\item Solve the equation:
\begin{equation}
Q(n-M,min(\chi^2)+\Delta\chi^2)=p
\end{equation}
\item Find the parameter region where $\chi^2\leq min(\chi^2)+\Delta\chi^2$. This define the confidence region.
\end{enumerate}
\subsection{Marginalization}

It is clear that a model can (in general) depend on more than one parameter. However, most of this parameters $\theta_i$ may be uninteresting. For example, this parameters can  corresponds with nuisance parameters like calibration factors or it may be that we are interested on constraints on only one parameter at the time rather than on the joint constraints on 2 or more parameters simultaneously. Then we marginalize over the uninteresting parameters by
\begin{equation}
P(\theta_1,...,\theta_j,H|D)=\int d\theta_{j+1}...d\theta_{m}P(\theta,H|D)
\end{equation}
where $m$ is the total number of parameters in our model and $\theta_1$,...,$\theta_j$ denote the parameters that we are interested in.

\subsection{Fisher Matrix}

Once we have a set of data it is important to know how accurate we can estimate model parameters. Fisher \cite{Fisher} proposed a way to solve this issue 70 years ago. In this section we review the main results of his work following the procedure given in \cite{LiV}.

First at all we consider again the gaussian likelihood. As we can notice the Heissian matrix $H_{\alpha\beta}$ enclose information of the parameters errors and their covariance. More specifically when all parameters are fixed except one (e.g. the ith parameter) the error on that parameter is $1/\sqrt{H_{ii}}$. These errors are called conditional errors, although they are not usually used.

A quantity that arises naturally with gaussian likelihoods to forecast the precision of a model is the so-called \textit{Fisher information matrix}
\begin{equation}
F_{ij}=-\left\langle \frac{\partial^2 \mathcal{L}}{\partial \theta_\alpha \partial \theta_\beta}\right\rangle
\end{equation}
where 
\begin{equation}
\mathcal{L}=\ln L
\end{equation}
It is clear that $F=\langle H\rangle$. Here average is over observational data. 

As we can see from eq. \eqref{rule4} when we have independent data sets, the complete likelihood is the product of the likelihoods and then the fisher matrix for independent data sets is the sum of the individual fisher matrices. 

A pedagogical and the easiest case is working in the one-parameter case $\theta_i$ and considering a gaussian likelihood. In this scenario we have that
\begin{equation}
\Delta \mathcal{L}=\frac{1}{2}F_{ii}(\theta_i- \theta_{0i})^2
\end{equation}
when $2\Delta\mathcal{L}=1$ and by identifying it with the $\Delta \chi^2$ corresponding to $68\%$ confidence level, we notice that $1/\sqrt{F_{ii}}$ yields the $1-\sigma$ displacement for $\theta_i$. In the general case
\begin{equation}\label{rao}
\sigma_{ij}^2 \geq (F^{-1})_{ij}
\end{equation}
Thus when all parameters are estimated simultaneously from the data the marginalized error is
\begin{equation}
\sigma_{\theta_i}\geq (F^{-1})^{1/2}_{ii}
\end{equation}
The beauty of the Fisher matrix approach is that there is a simple prescription for setting up the Fisher matrix knowing only your model and your measurement uncertainties and that
under the assumption of a gaussian likelihood, the Fisher matrix is the inverse of the covariance matrix. So all you have to do is set up the Fisher matrix and then invert it to obtain the covariance matrix (that is, the uncertainties on your model parameters). In addition if it can be computed quickly, it also enables one to explore different experimental set ups and optimize the experiment.

The whole point of the Fisher matrix formalism is to predict how well the experiment will be able to constrain the model parameters, before doing the experiment and in fact without even simulating the experiment in any detail. We can then forecast the results of different experiments and look at trade offs such as precision versus cost. In other words, we can engage in experimental design.

The $\leq$ in \eqref{rao} is called the Kramer-Rao inequality. We can see that the Fisher information matrix represent a lower bound for the errors. Only when the likelihood is gaussianly distributed the $\leq$ is transformed in $=$. However as we saw in [About the likelihood] a gaussian likelihood is only applicable to some circumstances, being generally impossible to be applied. So the key is to  have a good understanding of our theoretical model in such a way that we can construct a gaussian likelihood.
%\subsubsection{Computing Fisher matrices}


%\subsection{Summary}

%In order to make a Bayesian parameter inference we showed that the most important artifact necessary to know is the Bayes theorem. This theorem depend basically of 4 components: the prior, the likelihood, the posterior and the evidence. If we are only interested in the parameter inference procedure for a single model, we saw that we can ignore the posterior and the prior, being only the likelihood important for calculating the posterior (the posterior probability distribution for our parameter). Then, we can consider the most probable value for our parameter the one associated with the maximum value of our likelihood.
%
%In the other side we saw that there are some circumstances in which we can rewrite  

\section{Numerical tools}

In a typical scenario it is not possible to compute the posterior analytically in our Bayesian inference procedure. In this way it is important to know, once we have all the necessary mathematical tools to make a bayesian estimation, all the numerical tools at our disposal that can help us in our parameter estimation. Of course we could have several numerical candidates that could help us in this homework, but in this section we present only the typical one (and the easier) used in cosmology: the Monte Carlo Markov Chain (MCMC) with the Metropolis Hasting algorithm (MHA). Additionally in this section we present some useful details that we have to take into account if we want to make efficient our computation. 

\textit{\textbf{Important:}} In the next section we are going to apply, using a python code, all this new techniques to a cosmological example. The code is very simple and is programed using the PyMC module of Python. You can see it in appendix [B]. We present this code in order to make easier the homework for the reader that is starting with the parameter inference techniques and is interested in programing his/her own MCMC code. Notice that if you are not interested in a cosmological model, it is enough to change it in the function "model" by the one that you must prefer. 

\subsection{MCMC techniques for parameter inference}

The purpose of a MCMC algorithm is to construct a sequence of points in a parameter space (called ``chain") in order to evaluate the posterior of eq. \eqref{BayesT} for the usual case where analytical solutions do not exist or are insufficiently accurate. In this section we review the basics results for this procedure in a simplistic way but for curious readers it is recommendable to see \cite{mcmc1},\cite{mcmc2}, \cite{mcmc3} or only for markov chains \cite{mcmc4}.

A sequence $X_1,X_2,...$ of random elements of some set is a \textit{Markov chain} if the conditional distribution of $X_{n+1}$ given $X_1,...,X_n$ depends on $X_n$ only. In other words a Markov chain is a process where we can do predictions of the future based only in the information given at the present. An important property of a Markov chain is that they can be shown to converge to a stationary state were successive elements of the chain are samples from the target distribution, in our case the posterior $P(\theta,H|D)$. In this way we can estimate all the usual quantities of interest from it (mean, variance, etc). The number of points required to get good estimates is said to scale linearly with the number of parameters, so very quickly become much faster than grids as the dimensionality increases.

The target density is approximated by a set of delta function
\begin{equation}
p(\theta,H|D)\simeq \frac{1}{N}\sum_{i=1}^N \delta(\theta-\theta_i)
\end{equation}

In this procedure the posterior mean is given by
\begin{equation}
\langle\theta\rangle=\int d\theta \theta P(\theta,H|D)\simeq \frac{1}{N}\sum_{i=1}^N\theta_i
\end{equation}
where $\simeq$ follows because the sample $\theta_i$ are generated from the posterior by construction. Then, we can estimate any integrals (such as the mean, variance, etc) as
\begin{equation}
\langle f(\theta)\rangle \simeq\frac{1}{N}\sum_{i=1}^N f(\theta_i)
\end{equation}

As was mentioned, in a Markov chain it is necessary to generate a new point $\theta_{i+1}$ in our chain from the present point $\theta_i$. However, we would like to accept this point as a new point in the chain with a probability which depends on the ratio of the new and old target densities. The distribution of steps is called the \textit{proposal distribution}. In this way what we need now is an algorithm which can help us to accept/refuse the new points. The most popular algorithm is the so-called Metropolis Hasting algorithm and in what follows we explain in what it consists.
\subsubsection{Metropolis Hasting algorithm}

The \textit{Metropolis Hasting algorithm} (MHA) \cite{metr} is the simplest MCMC algorithm. For it, starting from a random initial point $\theta_0$, with associated posterior probability $p_0=p(\theta_0,H|D)$, we need to propose a candidate $\theta_c$ by drawing from the \textit{proposal distribution} $q(\theta_0,\theta_c)$. Then, the probability of acceptance of a new point is given by
\begin{equation}
p(acceptance)=min\left[1,\frac{p_cq(\theta_c,\theta_0)}{p_0q(\theta_0,\theta_c)}\right]
\end{equation}
If the proposal distribution is symmetric, the algorithm is reduced to the \textit{Metropolis algorithm}
\begin{equation}
p(acceptance)=min\left[1,\frac{p_c}{p_0}\right]
\end{equation}
In this way the complete algorithm can be expressed following this steps:
\begin{enumerate}
\item Choose a random initial condition $\theta_0$ in parameter space and compute the posterior distribution
\item Generate a new candidate from a proposal distribution in the parameter space and compute the corresponding posterior distribution
\item Accept (or not) the new point with the help of the Metropolis Hasting algorithm
\item If the point is not accepted, repeat the previous point in the chain
\item Repeat step 2-4 until you have a chain large enough
\end{enumerate}

\subsubsection{A first example in parameter inference using a MCMC technique with a MHA}

In order to exemplify the numerical tools learned in this section, let us 
go back to the example of the coins seen in subsection [Bayes theorem, prior and posterior distributions]. Because we are interested that the reader understands what is the basic procedure given in this section, let us only try to estimate what is the value of $p$ (or region of values for $p$) that best matches our data (the 14 times that the coin was trough). For it we calculate the posterior distribution \eqref{ex4} using the MHA. 

As before we consider a Likelihood given by a binomial distribution \eqref{ex2} and a normal distributed prior \eqref{ex3} ($a=b=1$). As our first ``guest" for $p$ we take $p_0=0.1$. We generate a new candidate $p_c$ as $p_c=p_i+G(p_i,\sigma)$, where $G(p_i,\sigma)$ is our Gaussian proposal distribution centered at $p_i$ and with variance $\sigma=0.02$; $p_i$ is the current value of $p$ that for our first step is $p_i=p_0$. Then, we compute the Metropolis Hasting algorithm in a Python code as can be seen in appendix [A]. Our final result,  fig. \ref{posteriord}, was a posterior distribution that matches very well with the posterior that was calculated analytically. Notice that we have plotted our $95\%$ confidence regions (black line). 

%\begin{figure}[h!]
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=7cm]{posterior.png}
\captionof{figure}{\footnotesize{Posterior distribution for our example. We plot the prior distribution (blue), true posterior (dashed-red) and the posterior calculated by the MHA (red). We plot $95\%$ confidence region for the estimation of $p$.}}
\label{posteriord}
\end{minipage}\\
%\end{figure}

To complete the example we show in figure \ref{chain1} the Markov Chain generated by our code. We can notice that the chain oscillates around a middle value. This behavior is expected due to the fact that we have not enough data to constrain more accurately the value of $p$ and then there are several values for $p$ that can matches with enough precision our data.   

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=6cm]{chain1.png}
\captionof{figure}{\footnotesize{Markvov chain. We use $p_0=0.1$ as our first ``guest" for $p$. }}
\label{chain1}
\end{minipage}\\ $ $ \\

Note: In appendix [A] we computed our MCMC algorithm using an explicit code of the MCMC process. However, in Python there are some modules that can help us to simplify our life. For example PyMC is a Python module that implements statistical models and fitting algorithms, including the MCMC algorithm. It is convenient to learn how to use this module if you are interested in construct your own algorithm to constrain a(some) parameter(s).
\subsubsection{Some useful details}

\textit{About the proposal distribution.} The choice of a proposal distribution $q$ is crucial for the efficient exploration of the posterior. If the scale of $q$ is too small compared with the scale of the target in the sense that the typical jump is small, then the chain may take a very long time to explore the target distribution which imply that the algorithm will be very inefficient. In the other side, if the scale of $q$ is too large, the chain gets stuck and it does not jump very frequently. In our example we used a Gaussian distribution with a variance (step) $\sigma=0.02$. This value was taken because we explored, by hand, different values for $\sigma$ and we took the one that looked to approx more quickly to the real main value of $p$.   

In order to fix this issue in a more efficient way, it is recommendable to run an exploratory MCMC, compute the covariance matrix from the samples, and then re-run with this covariant matrix as the covariance of a multivariate Gaussian proposal distribution. Of course, this process can be computed a couple of times before to run the ``real" MCMC.\\

\textit{About the burn-in.} It is important to notice that when we start a chain we will have a region of points outside the stationary region where the chain converge (In our chain we could consider those points as the ones inside the ellipse in figure \ref{chain1}). This early part of the chain (called ``bourn-in") must be ignored and then the dependence on the starting point must be lost. For it, it is important to have a convergence test which can help us to know when the chain has converged.\\

\textit{Convergence test.} It is clear that we need a test to know when our chain has converged. However we need to be warned that the point in our chain is not in a "false convergent point" or a locally maximum point. In this way we need that our rule takes into account this possible difficulty. The simplest way (the informal way) to know if our chain is converging is running several chains starting with different proposal initial point for the parameter that we are interested about to estimate. Then, if we see, by eye, that all the chains looks to converge to a single region of the possible value for our parameter, we could consider that our chains are converging to that region. 

 Taking again the experiment of the coins, we can run several chains for the above example and try to estimate if the value (the region) of $p$ that we found is an sttionary value (region). In figure \ref{chain2} we plotted 5 different Markov chains with initial ``guest" condition $p=0.2,0.3,0.5,0.7,0.9$. As we expected from the analytical result, all the chains looks to concentrate near the same value.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=6cm]{chain2.png}
\captionof{figure}{\footnotesize{Multiple MCMC. We calculate 5 Markov chains to estimate convergence of our chains.}}
\label{chain2}
\end{minipage}


The above convergence method explained is very informal and we would like to have a better way to ensure that our result is correct. The classical test used for it is the \textit{Gelman-Rubin} (1992) convergence criterion. This is (following \cite{LicV2}, \cite{AlanH}) by starting $M$ chains with very different initial point and $N$ points per chain. Then if $\theta_i^j$ is a point in parameter space of position $i$ and chain $j$, we need to compute the mean of each chain 
\begin{equation}
\langle\theta^j\rangle =\frac{1}{N}\sum_{i=1}^N \theta_i^j
\end{equation}
and the mean of all the chains
\begin{equation}
\langle\theta\rangle =\frac{1}{NM}\sum_{i=1}^N\sum_{j=1}^M\theta_i^j.
\end{equation}
Then the chain-to-chain variance $B$ is
\begin{equation}
B=\frac{1}{M-1}\sum_{j=1}^M(\langle\theta^j\rangle-\langle\theta\rangle)^2
\end{equation}
and the average variance of each chain is
\begin{equation}
W=\frac{1}{M(N-1)}\sum_{i=1}^N\sum_{j=1}^M(\theta_i^j-\langle\theta^j\rangle)^2
\end{equation}
If our chains converge, $W$ and $B/N$ must agree. In fact we say that the chain converges when the quantity
\begin{equation}
\hat R=\frac{\frac{N-1}{N}W+B(1+\frac{1}{M})}{W},
\end{equation}
which is the ratio of the two estimates, approach to unity. A typical convergence criteria is when $\hat R<1.03$. 
\\

\textit{More samples.-}The generation of the elements in a Markov chain is probabilistic by construction and it depends on the algorithm that we are working with. The MHA is the easiest algorithm used in bayesian inference. However there are several other algorithms that can help us to fulfill our mission. For instance, some of the most popular and effective ones apart of the MHA are the Gibbs sampling (GB) (see e.g. \cite{gibbs1,gibbs2}), the Hamiltoninan Monte Carlo (see e.g. \cite{hamiltonian1,Hamiltonian2}) or the Importance sampling (see e.g. \cite{importance}).

\section{Bayesian statistic and Cosmology}

Along the new age of cosmology, observational experiments have been very helpful to confront, eliminate or refined several cosmological models. On this period it has been necessary to improve sensitivity to the experiments in order to achieve stronger constraints on the different models and model's parameters. For example, on CMB satellite experiments, there was a factor of 10 better in sensitive from \textit{COBE} to $WMAP$ and from $WMAP$ to Planck \cite{cmbex}.  On redshift surveys experiments which are interested to map the universe in redshift space, we can compare the 18,000 redshift bright galaxies that were measured by the CfA2 Redshift Survey on 1995 with the \textcolor{red}{[number]} of galaxies that the Sloan Digital Sky Survey (SDSS) measured on \textcolor{red}{(year)} \cite{observ}. In addition there are several new cosmological observations that pretend to update and make our data more precise and that we will need to confront with our models or model's parameters. Such experiments are, for example \textcolor{red}{(Nuevos experimentos. Explicarlos un poco. Aqu\'i entonces comenzar a hablar de que estos nuevos datos se pueden utilizar para constre\~nir par\'ametros).}

\subsection{A first example in parameter inference for Cosmology}

The simplest way to understand how all this concepts can be useful in cosmology is applying them to an example. We start considering the typical  

\subsection{Cosmological codes}

Now a days there are a lot of cosmological Boltzmann codes that are free on the web and can help us to prove our cosmological models. The most popular of them are: CMBFAST \cite{cmbfast1}, CMBEASY \cite{cmbeasy}, CAMB \cite{camb1} (some useful references for CAMB: \cite{camb2,camb3,camb4}) and CLASS \cite{class1} (a useful reference for CLASS: \cite{mont1}). All of them are used for calculating the linear CMB anisotropy spectra based on integration over the sources along the photon past light cone.

%\subsubsection{CMBFAST}

%As can be seen in the NASA web-page \cite{cmbfast1}, CMBFAST is a code, written by U. Seljak and M. Zaldarriaga, for calculating the linear cosmic microwave background (CMB) anisotropy spectra based on integration over the sources along the photon past light cone. In this approach the temperature anisotropy is written as a time integral over the product of a geometrical term and a source term. The code can be downloaded as an interface in \cite{cmbfast1} where the user introduce entry parameters required by CMBFAST and when it is run, some of the results are displayed graphically; all of the results are stored in files and made available for the user.

%\subsubsection{CAMB}

%CAMB is a code for anisotropies in the Microwave Background. It was written by Antony Lewis and Anthony Challinor in the f90 language and based on CMBFAST. It can be downloaded in \cite{camb1}. Some useful references for the use of this code can be found in \cite{camb2}, \cite{camb3} and \cite{camb4}.

%\subsubsection{CLASS}

%As can be seen in the wave-page of CLASS \cite{class1}, the purpose of CLASS is to simulate the evolution of linear perturbations in the universe and to compute CMB and large scale structure observables. Its name also comes from the fact that it is written in object-oriented style mimicking the notion of class. The code was written in the C language. It can be downloaded in \cite{class1} and some useful references for the use of the code can be found in \cite{mont1}.

\subsection{Statistical codes}

Once our cosmological model is established we need a statistical code which can help us to estimate the free parameters of our model. A first idea could be continuing programing our own MCMC code, but, as it is expected, while the number of free parameters of our model increases, it is more challenging to construct an efficient code. Fortunately there are several MCMC codes free to download on-line that can make this homework taking as our theory the cosmological codes showed above. In this section we review the most common of them.

\subsubsection{Monte Python}

Monte Python is a Monte Carlo code for Cosmological Parameter extraction  that can be downloaded in \cite{MP1}. It contains likelihood codes of most recent experiments, and interfaces with the Boltzmann code Class for computing the cosmological observables.

The code contains several sampling methods available: Metropolis-Hastings, Nested Sampling (through MultiNest), EMCEE (through CosmoHammer) and Importance Sampling. If you are interested to work with this parameter inference code you can get help in \cite{mont1} and \cite{MP2}.

\subsubsection{CosmoMC}

CosmoMC (to download \cite{cosmomc}) is a fortran 2003 MCMC engine for exploring cosmological parameter space. It contains Monte Carlo samples and inportance sampling. It containg likelihoods of most recent experiments, and interfaces with CAMB.

\subsubsection{SimpleMC}

SimpleMC is a MCMC code for cosmological parameter estimation where only expansion history matters. It was written by An≈æe Slosar and Jose Vazquez and can be downloaded on \cite{simplemc}

\subsection{Some examples}
\textcolor{red}{Examples of cosmology}
\subsection{Conclusions}
\appendix
\section{A. A simple MCMC python code}

Here we show our MCMC written in Python. This code is very simple and its propose is to help the reader to understand how to programing a MCMC code. However, if you are interested in more sophisticated algorithms you can see the PyMC module of python [ref]. We wrote our code using the jupyter notebook [ref] which is a excelent editor when we have a program no much extense.  

\begin{figure}[h!]
\includegraphics[height=2cm]{c1.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=2.339cm]{c2.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=8.85cm]{c3.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=5.21cm]{c4.png}
\end{figure}

\begin{thebibliography}{9}
\bibitem{debate} The Shapley - Curtis Debate in 1920, $https://apod.nasa.gov/diamond_jubilee/debate_1920.html$, visited on December 2017
\bibitem{bayeslecture}B.J.K. Kleijin; Bayesian statistic, lecture notes 2015
\bibitem{AlanH} Alan Heavens; Statistical techniques in cosmology; May 2010
\bibitem{RobT} Roberto Trotta; Bayes in the sky: Beyesian inference and model selection in cosmology; March, 2008
\bibitem{LiV} Licia Verde; Statistical methods in cosmology; Nov, 2009
\bibitem{RobTr}Roberto Trotta; Bayes Methods in Cosmology; Jan, 2017
\bibitem{Fisher}Fisher R.A. (1935) \textit{J. Roy. Stat. Soc.} \textbf{98}, 39
\bibitem{NR}Numerical Recipes
\bibitem{mcmc1}anner, M. (1993)
Tools for Statistical Inference, Method for
Exploration of Posterior Distributions and Likelihood Func-
tions.
\bibitem{mcmc2}Gilks, W., Richardson, S. and Spiegelhalter, D. (1996)
Markov Chain
Monte Carlo in Practice.
\bibitem{mcmc3}Gelman, A., Carlin, J., Stern, H and Rubin, D. (1995)
Bayesian Data
Analysis.
\bibitem{mcmc4}oss, Sheldon, (1989)
Introduction to Probability models 4th
Edit.
\bibitem{metr} Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E.: Equation of state
calculations by fast computing machines. J. Chem. Phys.
21
, 1087‚Äì1092 (1953)
\bibitem{LicV2} Verde L., astroph/0712.3028 (2007)
\textit{http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html}
\bibitem{observ}218.163.109.230 et al. (2004‚Äì2014); \textit{Observational cosmology -
30h course}.
\bibitem{gibbs1} A. Smith and G. Roberts, J. R. Statist. Soc. B
55
3‚Äì23 (1993).
\bibitem{gibbs2} Ilker Yildirim, Bayesian Inference: Gibbs Sampling, August 2012
\bibitem{hamiltonian1}K. M. Hanson, Markov Chain Monte Carlo posterior sampling with the Hamiltonian method, in M. Sonka and K. M. Hanson eds, Medical Imaging: Image Processing
Vol. 4322, Proc. SPIE, pp. 456‚Äì467.
\bibitem{Hamiltonian2} Radford M. Neal, MCMC using Hamiltonian dynamics, arXiv:1206.1901v1 [stat.CO]
\bibitem{importance} Surya T. Tokdar and Robert E. Kass, Importance Sampling: A review, DOI: 10.1002/wics.56
%
%
%
%
\bibitem{cmbex} K. N. Abazajian, K. Arnold, J. Austermann, B. A. Ben-
son, C. Bischoff, J. Bock, J. R. Bond, J. Borrill, E. Cal-
abrese,  J.  E.  Carlstrom,  et  al.,  ArXiv  e-prints  (2013),
1309.5383.
\bibitem{cmbfast1}$https://lambda.gsfc.nasa.gov/toolbox/tb_cmbfast_ov.cfm$
\bibitem{cmbeasy}$http://adsabs.harvard.edu/abs/2010ascl.soft07004D$
\bibitem{camb1}$http://camb.info/$
\bibitem{camb2}$http://camb.info/readme.html$
\bibitem{camb3}$http://cosmocoffee.info/viewforum.php?f=11$
\bibitem{camb4} $http://cosmologist.info/notes/CAMB.pdf$
\bibitem{class1} http://class-code.net/
\bibitem{mont1}Zumalacarregui Miguel; \textit{CLASS, hi class and Monte Python basics IFT School on Cosmology Tools}; March 11, 17.
\bibitem{MP1}\textit{http://baudren.github.io/montepython.html}
\bibitem{MP2}Benjamin Audren, \textit{Monte Python documentation, Release 2.2.0}; October 21, 2015
\bibitem{cosmomc}$http://cosmologist.info/cosmomc/$	
\bibitem{simplemc}$https://github.com/ja-vazquez/SimpleMC$


\end{thebibliography}
\end{document}

